{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsMultilane.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPGUKsj0a40G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d5431f-813c-411e-e8cb-3d7517585b82"
      },
      "source": [
        "!pip3 uninstall -y keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling Keras-2.4.3:\n",
            "  Successfully uninstalled Keras-2.4.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLI65SXetBIl",
        "outputId": "1f52013b-c498-480a-c8c0-c443084eb839"
      },
      "source": [
        "!pip3 install keras==2.1.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/89/58ee5f56a9c26957d97217db41780ebedca3154392cb903c3f8a08a52208/Keras-2.1.2-py2.py3-none-any.whl (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.2) (1.19.5)\n",
            "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 2.1.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3akWE5-RtFDi",
        "outputId": "98ade159-c9c6-4f7c-cfb9-6e594b96fb58"
      },
      "source": [
        "!pip3 uninstall -y tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.4.1:\n",
            "  Successfully uninstalled tensorflow-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB_W9gsTtJSn",
        "outputId": "a48fc485-4e0f-4f48-f1e7-1a1e3aeae07d"
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.14.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/67/559ca8408431c37ad3a17e859c8c291ea82f092354074baef482b98ffb7b/tensorflow_gpu-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (377.1MB)\n",
            "\u001b[K     |████████████████████████████████| 377.1MB 48kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.1.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 54.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.12.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (3.12.4)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14.0) (54.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.7.4.3)\n",
            "Installing collected packages: keras-applications, tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJpRfao-vkTc",
        "outputId": "71c9d8bd-fb82-4a5b-dd87-45d2a62323bf"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "KxUmTAdvt5cP",
        "outputId": "dd2d46a2-639a-4b95-99eb-afdf38ccb5ec"
      },
      "source": [
        "\"\"\"\n",
        "Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset, \n",
        "not just on MNIST.\n",
        "*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\n",
        "uncommenting them and commenting their counterparts.\n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "\"\"\"\n",
        "\n",
        "import keras.backend as K\n",
        "from keras import initializers, layers\n",
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Length, self).__init__(**kwargs)\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return K.sqrt(K.sum(K.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional \n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(Mask, self).__init__(**kwargs)\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        inputs_expand = K.expand_dims(inputs, 1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, self.W, [2, 3]), elems=inputs_tiled)\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, dim=1)\n",
        "\n",
        "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
        "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  # [None, 10, 16]\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding, name, count):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name=name)(inputs)\n",
        "    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name=name+'i'+ str(count))(output)\n",
        "    return layers.Lambda(squash, name=name+ 'j' +str(count))(outputs)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# The following is another way to implement primary capsule layer. This is much slower.\n",
        "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    outputs = []\n",
        "    for _ in range(n_channels):\n",
        "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
        "    outputs = layers.Concatenate(axis=1)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# The following is another way to implement primary capsule layer. This is much slower.\\n# Apply Conv2D `n_channels` times and concatenate all capsules\\ndef PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\\n    outputs = []\\n    for _ in range(n_channels):\\n        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\\n        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\\n    outputs = layers.Concatenate(axis=1)(outputs)\\n    return layers.Lambda(squash)(outputs)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkzlvevSt8Y5"
      },
      "source": [
        "import numpy as np\n",
        "from keras import layers, models, optimizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8D_ZvJwXfKK"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    Inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(Inputs)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv2')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv3')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv4')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"1\", count=1)\n",
        "\n",
        "    # primarycaps1 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"2\")\n",
        "\n",
        "    # capsule = layers.concatenate([primarycaps, primarycaps1], axis=-1)\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    # digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                            #  name='digitcaps')(primarycaps)\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                             name='digitcaps')(primarycaps)\n",
        "    \n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([Inputs], [out_caps])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(Inputs, [out_caps])\n",
        "\n",
        "    # manipulate model\n",
        "    # noise = layers.Input(shape=(n_class, 16))\n",
        "    # noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    # masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    # manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    # return train_model, eval_model, manipulate_model\n",
        "    return train_model,eval_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HNaxDQ_X94x"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9B4WmctYBIs",
        "outputId": "c900a170-a60f-4ed6-9f20-97003c8c766c"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=(256,256,3), n_class=2, routings=6)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3464: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-6-6040a09248db>:147: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 252, 252, 16)      1216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 126, 126, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 122, 122, 32)      12832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 61, 61, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 57, 57, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 20, 20, 128)       663680    \n",
            "_________________________________________________________________\n",
            "1 (Conv2D)                   (None, 6, 6, 256)         2654464   \n",
            "_________________________________________________________________\n",
            "1i1 (Reshape)                (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "1j1 (Lambda)                 (None, 1152, 8)           0         \n",
            "_________________________________________________________________\n",
            "digitcaps (CapsuleLayer)     (None, 2, 16)             294912    \n",
            "_________________________________________________________________\n",
            "capsnet (Length)             (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 3,678,368\n",
            "Trainable params: 3,678,368\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMIdYTgPYFB-",
        "outputId": "8fc16361-9e0f-407b-997e-02ff4414fca3"
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOqUMaFAYIlf"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import tempfile\n",
        "\n",
        "def _copy_symlinks(files, src_dir, dst_dir):\n",
        "    for i in files:\n",
        "        base_file_name = os.path.basename(i)\n",
        "        src_file_path = os.path.join(src_dir, base_file_name)\n",
        "        dst_file_path = os.path.join(dst_dir, base_file_name)\n",
        "        src_file_path = os.path.abspath(src_file_path)\n",
        "        dst_file_path = os.path.abspath(dst_file_path)\n",
        "        os.symlink(src_file_path, dst_file_path)\n",
        "\n",
        "def train_valid_split(original_dir, validation_split=0.1, seed=None):\n",
        "    if seed is not None:\n",
        "        random.seed(seed)    \n",
        "    if not os.path.isdir(original_dir):\n",
        "        raise NotADirectoryError\n",
        "    tmp_dir = tempfile.TemporaryDirectory()\n",
        "    train_dir = os.path.join(tmp_dir.name, 'train')\n",
        "    valid_dir = os.path.join(tmp_dir.name, 'validation')\n",
        "\n",
        "    # make subdirs in train tmp and valid tmp\n",
        "    for root, dirs, files in os.walk(original_dir):\n",
        "        if root == original_dir:\n",
        "            continue\n",
        "        sub_dir_name = os.path.basename(root)\n",
        "        train_sub_dir_path = os.path.join(train_dir, sub_dir_name)\n",
        "        valid_sub_dir_path = os.path.join(valid_dir, sub_dir_name)\n",
        "        if not os.path.exists(train_sub_dir_path):\n",
        "            os.makedirs(train_sub_dir_path)\n",
        "        if not os.path.exists(valid_sub_dir_path):\n",
        "            os.makedirs(valid_sub_dir_path)\n",
        "\n",
        "    # distribute symlinks to train_tmp, test_tmp\n",
        "    for root, dirs, files in os.walk(original_dir):\n",
        "        if root == original_dir:\n",
        "            continue\n",
        "        sub_dir_name = os.path.basename(root)\n",
        "        train_sub_dir_path = os.path.join(train_dir, sub_dir_name)\n",
        "        valid_sub_dir_path = os.path.join(valid_dir, sub_dir_name)\n",
        "        files = [os.path.join(root, f) for f in files]\n",
        "        random.shuffle(files)\n",
        "        valid_idx = math.ceil(validation_split * len(files))\n",
        "        train_files = files[valid_idx:]\n",
        "        valid_files = files[:valid_idx]\n",
        "        _copy_symlinks(train_files, root, train_sub_dir_path)\n",
        "        _copy_symlinks(valid_files, root, valid_sub_dir_path)\n",
        "    return tmp_dir, train_dir, valid_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoDK7TTHYL66",
        "outputId": "8d34e536-918c-4db1-b5a3-a1ad69a8fdc5"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,\n",
        "                    epochs=15)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_1_capsule_epoch15.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch 1/15\n",
            "61/61 [==============================] - 504s 8s/step - loss: 0.8164 - acc: 0.6036\n",
            "Epoch 2/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.6380 - acc: 0.6618\n",
            "Epoch 3/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.5697 - acc: 0.6988\n",
            "Epoch 4/15\n",
            "61/61 [==============================] - 61s 999ms/step - loss: 0.4433 - acc: 0.8045\n",
            "Epoch 5/15\n",
            "61/61 [==============================] - 61s 1000ms/step - loss: 0.3663 - acc: 0.8561\n",
            "Epoch 6/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.3363 - acc: 0.8735\n",
            "Epoch 7/15\n",
            "61/61 [==============================] - 61s 998ms/step - loss: 0.2818 - acc: 0.8991\n",
            "Epoch 8/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2796 - acc: 0.8987\n",
            "Epoch 9/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2073 - acc: 0.9335\n",
            "Epoch 10/15\n",
            "61/61 [==============================] - 63s 1s/step - loss: 0.2025 - acc: 0.9295\n",
            "Epoch 11/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1824 - acc: 0.9392\n",
            "Epoch 12/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1638 - acc: 0.9443\n",
            "Epoch 13/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1578 - acc: 0.9524\n",
            "Epoch 14/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1370 - acc: 0.9570\n",
            "Epoch 15/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1565 - acc: 0.9466\n",
            "[[407  13]\n",
            " [ 13 213]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97       420\n",
            "           1       0.94      0.94      0.94       226\n",
            "\n",
            "    accuracy                           0.96       646\n",
            "   macro avg       0.96      0.96      0.96       646\n",
            "weighted avg       0.96      0.96      0.96       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK2ruz2vfV-5"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    Inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(Inputs)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv2')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv3')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv4')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"1\", count=1)\n",
        "\n",
        "    primarycaps1 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"2\", count=2)\n",
        "\n",
        "    capsule = layers.concatenate([primarycaps, primarycaps1], axis=-1)\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    # digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                            #  name='digitcaps')(primarycaps)\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                             name='digitcaps')(capsule)\n",
        "    \n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([Inputs], [out_caps])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(Inputs, [out_caps])\n",
        "\n",
        "    # manipulate model\n",
        "    # noise = layers.Input(shape=(n_class, 16))\n",
        "    # noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    # masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    # manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    # return train_model, eval_model, manipulate_model\n",
        "    return train_model,eval_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeAdqHrRfoen",
        "outputId": "8401b042-290e-47dd-bc13-b360a416e423"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=(256,256,3), n_class=2, routings=6)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:64: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:497: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3636: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3464: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From <ipython-input-2-6040a09248db>:147: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 252, 252, 16) 1216        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 126, 126, 16) 0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 122, 122, 32) 12832       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 61, 61, 32)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 57, 57, 64)   51264       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 28, 28, 64)   0           conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 20, 20, 128)  663680      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "1 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "2 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1i1 (Reshape)                   (None, 1152, 8)      0           1[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "2i2 (Reshape)                   (None, 1152, 8)      0           2[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "1j1 (Lambda)                    (None, 1152, 8)      0           1i1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "2j2 (Lambda)                    (None, 1152, 8)      0           2i2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1152, 16)     0           1j1[0][0]                        \n",
            "                                                                 2j2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (None, 2, 16)        589824      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 2)            0           digitcaps[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,627,744\n",
            "Trainable params: 6,627,744\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tto6rtLf23R",
        "outputId": "844713a5-b590-423f-cf8a-c09d14db4513"
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2950: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-zw-RYWf3zU",
        "outputId": "38dbb876-0b51-483c-e748-988746734e3d"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,\n",
        "                    epochs=15)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_2_capsule_epoch15.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch 1/15\n",
            "61/61 [==============================] - 63s 1s/step - loss: 0.8176 - acc: 0.6190\n",
            "Epoch 2/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.5338 - acc: 0.7271\n",
            "Epoch 3/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.4183 - acc: 0.8284\n",
            "Epoch 4/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.3603 - acc: 0.8532\n",
            "Epoch 5/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.2735 - acc: 0.8979\n",
            "Epoch 6/15\n",
            "61/61 [==============================] - 61s 998ms/step - loss: 0.2266 - acc: 0.9144\n",
            "Epoch 7/15\n",
            "61/61 [==============================] - 61s 998ms/step - loss: 0.2119 - acc: 0.9290\n",
            "Epoch 8/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.2098 - acc: 0.9274\n",
            "Epoch 9/15\n",
            "61/61 [==============================] - 61s 997ms/step - loss: 0.1598 - acc: 0.9480\n",
            "Epoch 10/15\n",
            "61/61 [==============================] - 61s 997ms/step - loss: 0.1701 - acc: 0.9431\n",
            "Epoch 11/15\n",
            "61/61 [==============================] - 61s 992ms/step - loss: 0.1490 - acc: 0.9498\n",
            "Epoch 12/15\n",
            "61/61 [==============================] - 61s 992ms/step - loss: 0.1275 - acc: 0.9562\n",
            "Epoch 13/15\n",
            "61/61 [==============================] - 61s 994ms/step - loss: 0.1267 - acc: 0.9593\n",
            "Epoch 14/15\n",
            "61/61 [==============================] - 60s 992ms/step - loss: 0.1182 - acc: 0.9613\n",
            "Epoch 15/15\n",
            "61/61 [==============================] - 61s 998ms/step - loss: 0.1048 - acc: 0.9641\n",
            "[[415   5]\n",
            " [ 19 207]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.97       420\n",
            "           1       0.98      0.92      0.95       226\n",
            "\n",
            "    accuracy                           0.96       646\n",
            "   macro avg       0.97      0.95      0.96       646\n",
            "weighted avg       0.96      0.96      0.96       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbPUxI6cxZld"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    Inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(Inputs)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv2')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv3')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv4')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"1\", count=1)\n",
        "\n",
        "    primarycaps1 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"2\", count=2)\n",
        "\n",
        "    primarycaps2 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"3\", count=3)\n",
        "\n",
        "    capsule = layers.concatenate([primarycaps, primarycaps1, primarycaps2], axis=-1)\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    # digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                            #  name='digitcaps')(primarycaps)\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=8, routings=routings,\n",
        "                             name='digitcaps')(capsule)\n",
        "    \n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([Inputs], [out_caps])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(Inputs, [out_caps])\n",
        "\n",
        "    # manipulate model\n",
        "    # noise = layers.Input(shape=(n_class, 16))\n",
        "    # noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    # masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    # manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    # return train_model, eval_model, manipulate_model\n",
        "    return train_model,eval_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KOvyxPrxmfE",
        "outputId": "4eac05cc-8e5e-4d22-9ea7-31bfcf1bbb0c"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=(256,256,3), n_class=2, routings=6)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 252, 252, 16) 1216        input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 126, 126, 16) 0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 122, 122, 32) 12832       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 61, 61, 32)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 57, 57, 64)   51264       max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 28, 28, 64)   0           conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 20, 20, 128)  663680      max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "1 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "2 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "3 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1i1 (Reshape)                   (None, 1152, 8)      0           1[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "2i2 (Reshape)                   (None, 1152, 8)      0           2[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "3i3 (Reshape)                   (None, 1152, 8)      0           3[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "1j1 (Lambda)                    (None, 1152, 8)      0           1i1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "2j2 (Lambda)                    (None, 1152, 8)      0           2i2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "3j3 (Lambda)                    (None, 1152, 8)      0           3i3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1152, 24)     0           1j1[0][0]                        \n",
            "                                                                 2j2[0][0]                        \n",
            "                                                                 3j3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (None, 2, 8)         442368      concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 2)            0           digitcaps[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 9,134,752\n",
            "Trainable params: 9,134,752\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2bMYCQ4xi_D"
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcKTopCtxpAb",
        "outputId": "40e1ef2d-f335-46df-d4a6-3818b82dc2cf"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,\n",
        "                    epochs=15)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_3_capsule_epoch15.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 1.0744 - acc: 0.5580\n",
            "Epoch 2/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.6738 - acc: 0.6370\n",
            "Epoch 3/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.5252 - acc: 0.7336\n",
            "Epoch 4/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.4365 - acc: 0.8134\n",
            "Epoch 5/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.3440 - acc: 0.8757\n",
            "Epoch 6/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.3371 - acc: 0.8697\n",
            "Epoch 7/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2668 - acc: 0.9036\n",
            "Epoch 8/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2764 - acc: 0.9005\n",
            "Epoch 9/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2260 - acc: 0.9241\n",
            "Epoch 10/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2166 - acc: 0.9260\n",
            "Epoch 11/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2033 - acc: 0.9334\n",
            "Epoch 12/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1852 - acc: 0.9365\n",
            "Epoch 13/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1975 - acc: 0.9343\n",
            "Epoch 14/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1907 - acc: 0.9373\n",
            "Epoch 15/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1432 - acc: 0.9556\n",
            "[[399  21]\n",
            " [  2 224]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97       420\n",
            "           1       0.91      0.99      0.95       226\n",
            "\n",
            "    accuracy                           0.96       646\n",
            "   macro avg       0.95      0.97      0.96       646\n",
            "weighted avg       0.97      0.96      0.96       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX0bfmN59BAJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFF2mA6a9Lj6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVLuO9439LsF"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    Inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(Inputs)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv2')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv3')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv4')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name=\"1\", count=1)\n",
        "\n",
        "    primarycaps1 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"2\", count=2)\n",
        "\n",
        "    primarycaps2 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"3\", count=3)\n",
        "\n",
        "    primarycaps3 = PrimaryCap(x, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid', name = \"4\", count=4)\n",
        "\n",
        "    capsule = layers.concatenate([primarycaps, primarycaps1, primarycaps2, primarycaps3], axis=-1)\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    # digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                            #  name='digitcaps')(primarycaps)\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=8, routings=routings,\n",
        "                             name='digitcaps')(capsule)\n",
        "    \n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(digitcaps)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([Inputs], [out_caps])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(Inputs, [out_caps])\n",
        "\n",
        "    # manipulate model\n",
        "    # noise = layers.Input(shape=(n_class, 16))\n",
        "    # noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    # masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    # manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    # return train_model, eval_model, manipulate_model\n",
        "    return train_model,eval_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcYCK6ak9Gl3",
        "outputId": "6660f06a-434f-496f-f420-0451359ddadd"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=(256,256,3), n_class=2, routings=6)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 252, 252, 16) 1216        input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 126, 126, 16) 0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 122, 122, 32) 12832       max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 61, 61, 32)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 57, 57, 64)   51264       max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 28, 28, 64)   0           conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 20, 20, 128)  663680      max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "1 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "2 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "3 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "4 (Conv2D)                      (None, 6, 6, 256)    2654464     conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "1i1 (Reshape)                   (None, 1152, 8)      0           1[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "2i2 (Reshape)                   (None, 1152, 8)      0           2[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "3i3 (Reshape)                   (None, 1152, 8)      0           3[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "4i4 (Reshape)                   (None, 1152, 8)      0           4[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "1j1 (Lambda)                    (None, 1152, 8)      0           1i1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "2j2 (Lambda)                    (None, 1152, 8)      0           2i2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "3j3 (Lambda)                    (None, 1152, 8)      0           3i3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "4j4 (Lambda)                    (None, 1152, 8)      0           4i4[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 1152, 32)     0           1j1[0][0]                        \n",
            "                                                                 2j2[0][0]                        \n",
            "                                                                 3j3[0][0]                        \n",
            "                                                                 4j4[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps (CapsuleLayer)        (None, 2, 8)         589824      concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 2)            0           digitcaps[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 11,936,672\n",
            "Trainable params: 11,936,672\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbkIx6kY9Gl4"
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZREtUhcS9Gl5",
        "outputId": "27145a58-1944-4e6d-910a-3e62be8e8a9c"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,\n",
        "                    epochs=15)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_4_capsule_epoch15.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.8839 - acc: 0.6141\n",
            "Epoch 2/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.4741 - acc: 0.7856\n",
            "Epoch 3/15\n",
            "61/61 [==============================] - 61s 1000ms/step - loss: 0.3680 - acc: 0.8524\n",
            "Epoch 4/15\n",
            "61/61 [==============================] - 61s 997ms/step - loss: 0.3298 - acc: 0.8678\n",
            "Epoch 5/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2656 - acc: 0.9055\n",
            "Epoch 6/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2480 - acc: 0.9129\n",
            "Epoch 7/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.2039 - acc: 0.9315\n",
            "Epoch 8/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1720 - acc: 0.9482\n",
            "Epoch 9/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1726 - acc: 0.9487\n",
            "Epoch 10/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1611 - acc: 0.9465\n",
            "Epoch 11/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1382 - acc: 0.9529\n",
            "Epoch 12/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1740 - acc: 0.9424\n",
            "Epoch 13/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1379 - acc: 0.9552\n",
            "Epoch 14/15\n",
            "61/61 [==============================] - 61s 1s/step - loss: 0.1289 - acc: 0.9557\n",
            "Epoch 15/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.1210 - acc: 0.9607\n",
            "[[415   5]\n",
            " [ 27 199]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.96       420\n",
            "           1       0.98      0.88      0.93       226\n",
            "\n",
            "    accuracy                           0.95       646\n",
            "   macro avg       0.96      0.93      0.94       646\n",
            "weighted avg       0.95      0.95      0.95       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV46A6-zKwqA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "TqIG50MoSCGR",
        "outputId": "5b35e918-0d4a-4e2e-d9a0-080c17df5dee"
      },
      "source": [
        "!pip3 install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 20.6MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 28.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 21.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 19.3MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 16.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 15.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 16.0MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 14.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "\u001b[31mERROR: fancyimpute 0.4.3 requires tensorflow, which is not installed.\u001b[0m\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93g6rcbkKw1s"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "from keras.layers import Dense, Flatten\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    Inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(Inputs)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv2')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv3')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv4')(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    \n",
        "    out_caps = Dense(2, activation='sigmoid')(x)\n",
        "\n",
        "    \n",
        "    train_model = models.Model([Inputs], [out_caps])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(Inputs, [out_caps])\n",
        "\n",
        "    # manipulate model\n",
        "    # noise = layers.Input(shape=(n_class, 16))\n",
        "    # noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    # masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    # manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    # return train_model, eval_model, manipulate_model\n",
        "    return train_model,eval_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLtb7TtvK6JE",
        "outputId": "dbc70cb4-dd39-4065-c3e0-29facca1b7ea"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=(256,256,3), n_class=2, routings=6)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 252, 252, 16)      1216      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 126, 126, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 122, 122, 32)      12832     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 61, 61, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 57, 57, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 20, 20, 128)       663680    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 102402    \n",
            "=================================================================\n",
            "Total params: 831,394\n",
            "Trainable params: 831,394\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rAS_Ob1LEY5"
      },
      "source": [
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leBgZgbWLKSp",
        "outputId": "e3cbefef-6cd1-4d7d-ce09-9d6467279a81"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,\n",
        "                    epochs=15)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/cnnCovid_epoch15.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "61/61 [==============================] - 62s 1s/step - loss: 0.6249 - acc: 0.7103\n",
            "Epoch 2/15\n",
            "61/61 [==============================] - 60s 982ms/step - loss: 0.3816 - acc: 0.8563\n",
            "Epoch 3/15\n",
            "61/61 [==============================] - 60s 978ms/step - loss: 0.3685 - acc: 0.8586\n",
            "Epoch 4/15\n",
            "61/61 [==============================] - 60s 984ms/step - loss: 0.3064 - acc: 0.8815\n",
            "Epoch 5/15\n",
            "61/61 [==============================] - 60s 981ms/step - loss: 0.2404 - acc: 0.9081\n",
            "Epoch 6/15\n",
            "61/61 [==============================] - 60s 979ms/step - loss: 0.2246 - acc: 0.9085\n",
            "Epoch 7/15\n",
            "61/61 [==============================] - 60s 979ms/step - loss: 0.2061 - acc: 0.9232\n",
            "Epoch 8/15\n",
            "61/61 [==============================] - 59s 972ms/step - loss: 0.1852 - acc: 0.9306\n",
            "Epoch 9/15\n",
            "61/61 [==============================] - 59s 974ms/step - loss: 0.1653 - acc: 0.9426\n",
            "Epoch 10/15\n",
            "61/61 [==============================] - 59s 973ms/step - loss: 0.1754 - acc: 0.9401\n",
            "Epoch 11/15\n",
            "61/61 [==============================] - 59s 974ms/step - loss: 0.1713 - acc: 0.9342\n",
            "Epoch 12/15\n",
            "61/61 [==============================] - 59s 974ms/step - loss: 0.1446 - acc: 0.9490\n",
            "Epoch 13/15\n",
            "61/61 [==============================] - 59s 974ms/step - loss: 0.1210 - acc: 0.9568\n",
            "Epoch 14/15\n",
            "61/61 [==============================] - 59s 975ms/step - loss: 0.1383 - acc: 0.9483\n",
            "Epoch 15/15\n",
            "61/61 [==============================] - 60s 981ms/step - loss: 0.1310 - acc: 0.9529\n",
            "[[400  20]\n",
            " [  8 218]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97       420\n",
            "           1       0.92      0.96      0.94       226\n",
            "\n",
            "    accuracy                           0.96       646\n",
            "   macro avg       0.95      0.96      0.95       646\n",
            "weighted avg       0.96      0.96      0.96       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afGWady2RlT9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzlYEddpRlvP"
      },
      "source": [
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers, initializers, regularizers, constraints\n",
        "from keras.utils import conv_utils\n",
        "from keras.layers import InputSpec\n",
        "from keras.utils.conv_utils import conv_output_length\n",
        "import keras.backend.tensorflow_backend as tfback\n",
        "\n",
        "from keras.backend import *\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "\n",
        "#own_batch_dot = batch_dot  # force standard implementation \n",
        "\n",
        "# import of batch_dot operation from TF 1.13\n",
        "# https://github.com/tensorflow/tensorflow/blob/v1.13.1/tensorflow/python/keras/backend.py\n",
        "\n",
        "def own_batch_dot(x, y, axes=None):\n",
        "    \"\"\"Batchwise dot product.\n",
        "    `batch_dot` is used to compute dot product of `x` and `y` when\n",
        "    `x` and `y` are data in batch, i.e. in a shape of\n",
        "    `(batch_size, :)`.\n",
        "    `batch_dot` results in a tensor or variable with less dimensions\n",
        "    than the input. If the number of dimensions is reduced to 1,\n",
        "    we use `expand_dims` to make sure that ndim is at least 2.\n",
        "    Arguments:\n",
        "        x: Keras tensor or variable with `ndim >= 2`.\n",
        "        y: Keras tensor or variable with `ndim >= 2`.\n",
        "        axes: list of (or single) int with target dimensions.\n",
        "            The lengths of `axes[0]` and `axes[1]` should be the same.\n",
        "    Returns:\n",
        "        A tensor with shape equal to the concatenation of `x`'s shape\n",
        "        (less the dimension that was summed over) and `y`'s shape\n",
        "        (less the batch dimension and the dimension that was summed over).\n",
        "        If the final rank is 1, we reshape it to `(batch_size, 1)`.\n",
        "    Examples:\n",
        "        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n",
        "        `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n",
        "        of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n",
        "        elements.\n",
        "        Shape inference:\n",
        "        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n",
        "        If `axes` is (1, 2), to find the output shape of resultant tensor,\n",
        "            loop through each dimension in `x`'s shape and `y`'s shape:\n",
        "        * `x.shape[0]` : 100 : append to output shape\n",
        "        * `x.shape[1]` : 20 : do not append to output shape,\n",
        "            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n",
        "        * `y.shape[0]` : 100 : do not append to output shape,\n",
        "            always ignore first dimension of `y`\n",
        "        * `y.shape[1]` : 30 : append to output shape\n",
        "        * `y.shape[2]` : 20 : do not append to output shape,\n",
        "            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n",
        "        `output_shape` = `(100, 30)`\n",
        "    ```python\n",
        "        >>> x_batch = K.ones(shape=(32, 20, 1))\n",
        "        >>> y_batch = K.ones(shape=(32, 30, 20))\n",
        "        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n",
        "        >>> K.int_shape(xy_batch_dot)\n",
        "        (32, 1, 30)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    if isinstance(axes, int):\n",
        "        axes = (axes, axes)\n",
        "    x_ndim = ndim(x)\n",
        "    y_ndim = ndim(y)\n",
        "    if axes is None:\n",
        "      # behaves like tf.batch_matmul as default\n",
        "        axes = [x_ndim - 1, y_ndim - 2]\n",
        "    if x_ndim > y_ndim:\n",
        "        diff = x_ndim - y_ndim\n",
        "        y = array_ops.reshape(y,\n",
        "                            array_ops.concat(\n",
        "                                [array_ops.shape(y), [1] * (diff)], axis=0))\n",
        "    elif y_ndim > x_ndim:\n",
        "        diff = y_ndim - x_ndim\n",
        "        x = array_ops.reshape(x,\n",
        "                            array_ops.concat(\n",
        "                                [array_ops.shape(x), [1] * (diff)], axis=0))\n",
        "    else:\n",
        "        diff = 0\n",
        "    if ndim(x) == 2 and ndim(y) == 2:\n",
        "        if axes[0] == axes[1]:\n",
        "          out = math_ops.reduce_sum(math_ops.multiply(x, y), axes[0])\n",
        "        else:\n",
        "          out = math_ops.reduce_sum(math_ops.multiply(array_ops.transpose(x, [1, 0]), y), axes[1])\n",
        "    else:\n",
        "        adj_x = None if axes[0] == ndim(x) - 1 else True\n",
        "        adj_y = True if axes[1] == ndim(y) - 1 else None\n",
        "        if adj_x==True:\n",
        "          x = tf.transpose(x, conjugate=True)\n",
        "        if adj_y==True:\n",
        "          y = tf.transpose(y, conjugate=True)\n",
        "        # out = math_ops.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n",
        "        out = tf.tensordot(x, y, axes=1)\n",
        "    if diff:\n",
        "        if x_ndim > y_ndim:\n",
        "          idx = x_ndim + y_ndim - 3\n",
        "        else:\n",
        "          idx = x_ndim - 1\n",
        "        out = array_ops.squeeze(out, list(range(idx, idx + diff)))\n",
        "    if ndim(out) == 1:\n",
        "        out = expand_dims(out, 1)\n",
        "    return out\n",
        "\n",
        "def _get_available_gpus():\n",
        "    \"\"\"Get a list of available gpu devices (formatted as strings).\n",
        "\n",
        "    # Returns\n",
        "        A list of available GPU devices.\n",
        "    \"\"\"\n",
        "    \n",
        "    return [tf.test.gpu_device_name()]\n",
        "\n",
        "tfback._get_available_gpus = _get_available_gpus\n",
        "\n",
        "cf = K.image_data_format() == '..'\n",
        "useGPU = True\n",
        "\n",
        "\n",
        "def squeeze(s):\n",
        "    sq = K.sum(K.square(s), axis=-1, keepdims=True)\n",
        "    return (sq / (1 + sq)) * (s / K.sqrt(sq + K.epsilon()))\n",
        "\n",
        "\n",
        "class ConvertToCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ConvertToCaps, self).__init__(**kwargs)\n",
        "        # self.input_spec = InputSpec(min_ndim=2)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape.insert(1 if cf else len(output_shape), 1)\n",
        "        return tuple(output_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return K.expand_dims(inputs, 1 if cf else -1)\n",
        "\n",
        "    def get_config(self):\n",
        "        # config = {\n",
        "        #     'input_spec': 5\n",
        "        # }\n",
        "        base_config = super(ConvertToCaps, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "\n",
        "class FlattenCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(FlattenCaps, self).__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(min_ndim=4)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # if not all(input_shape[1:]):\n",
        "        #     raise ValueError('The shape of the input to \"FlattenCaps\" '\n",
        "        #                      'is not fully defined '\n",
        "        #                      '(got ' + str(input_shape[1:]) + '. '\n",
        "        #                      'Make sure to pass a complete \"input_shape\" '\n",
        "        #                      'or \"batch_input_shape\" argument to the first '\n",
        "        #                      'layer in your model.')\n",
        "        return (input_shape[0], np.prod(input_shape[1:-1]), input_shape[-1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        shape = K.int_shape(inputs)\n",
        "        return K.reshape(inputs, (-1, np.prod(shape[1:-1]), shape[-1]))\n",
        "    def get_config(self):\n",
        "        # config = {\n",
        "        #       'input_spec': 5\n",
        "        #   }\n",
        "        base_config = super(FlattenCaps, self).get_config()\n",
        "        # return dict(list(base_config.items()) + list(config.items()))\n",
        "        return base_config\n",
        "\n",
        "\n",
        "class CapsToScalars(layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CapsToScalars, self).__init__(**kwargs)\n",
        "        self.input_spec = InputSpec(min_ndim=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return K.sqrt(K.sum(K.square(inputs + K.epsilon()), axis=-1))\n",
        "    \n",
        "    def get_config(self):\n",
        "        # config = {\n",
        "        #       'input_spec': 5\n",
        "        #   }\n",
        "        base_config = super(CapsToScalars, self).get_config()\n",
        "        # return dict(list(base_config.items()) + list(config.items()))\n",
        "        return base_config\n",
        "\n",
        "class Conv2DCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, ch_j, n_j,\n",
        "                 kernel_size=(3, 3),\n",
        "                 strides=(1, 1),\n",
        "                 r_num=1,\n",
        "                 b_alphas=[8, 8, 8],\n",
        "                 padding='same',\n",
        "                 data_format='channels_last',\n",
        "                 dilation_rate=(1, 1),\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(Conv2DCaps, self).__init__(**kwargs)\n",
        "        rank = 2\n",
        "        self.ch_j = ch_j  # Number of capsules in layer J\n",
        "        self.n_j = n_j  # Number of neurons in a capsule in J\n",
        "        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n",
        "        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n",
        "        self.r_num = r_num\n",
        "        self.b_alphas = b_alphas\n",
        "        self.padding = conv_utils.normalize_padding(padding)\n",
        "        self.data_format = K.normalize_data_format(data_format)\n",
        "        self.dilation_rate = (1, 1)\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.input_spec = InputSpec(ndim=rank + 3)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        self.h_i, self.w_i, self.ch_i, self.n_i = input_shape[1:5]\n",
        "\n",
        "        self.h_j, self.w_j = [conv_utils.conv_output_length(input_shape[i + 1],\n",
        "                                                            self.kernel_size[i],\n",
        "                                                            padding=self.padding,\n",
        "                                                            stride=self.strides[i],\n",
        "                                                            dilation=self.dilation_rate[i]) for i in (0, 1)]\n",
        "\n",
        "        self.ah_j, self.aw_j = [conv_utils.conv_output_length(input_shape[i + 1],\n",
        "                                                              self.kernel_size[i],\n",
        "                                                              padding=self.padding,\n",
        "                                                              stride=1,\n",
        "                                                              dilation=self.dilation_rate[i]) for i in (0, 1)]\n",
        "\n",
        "        self.w_shape = self.kernel_size + (self.ch_i, self.n_i,\n",
        "                                           self.ch_j, self.n_j)\n",
        "\n",
        "        self.w = self.add_weight(shape=self.w_shape,\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.kernel_regularizer,\n",
        "                                 constraint=self.kernel_constraint)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.r_num == 1:\n",
        "            # if there is no routing (and this is so when r_num is 1 and all c are equal)\n",
        "            # then this is a common convolution\n",
        "            outputs = K.conv2d(K.reshape(inputs, (-1, self.h_i, self.w_i,\n",
        "                                                  self.ch_i * self.n_i)),\n",
        "                               K.reshape(self.w, self.kernel_size +\n",
        "                                         (self.ch_i * self.n_i, self.ch_j * self.n_j)),\n",
        "                               data_format='channels_last',\n",
        "                               strides=self.strides,\n",
        "                               padding=self.padding,\n",
        "                               dilation_rate=self.dilation_rate)\n",
        "\n",
        "            outputs = squeeze(K.reshape(outputs, ((-1, self.h_j, self.w_j,\n",
        "                                                   self.ch_j, self.n_j))))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.h_j, self.w_j, self.ch_j, self.n_j)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'ch_j': self.ch_j,\n",
        "            'n_j': self.n_j,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'strides': self.strides,\n",
        "            'b_alphas': self.b_alphas,\n",
        "            'padding': self.padding,\n",
        "            'data_format': self.data_format,\n",
        "            'dilation_rate': self.dilation_rate,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
        "            'kernel_constraint': constraints.serialize(self.kernel_constraint)\n",
        "        }\n",
        "        base_config = super(Conv2DCaps, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.one_hot(indices=K.argmax(x, 1), num_classes=x.get_shape().as_list()[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * K.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape[0], tuple):  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "    def get_config(self):\n",
        "        \n",
        "        base_config = super(Mask, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "\n",
        "class Mask_CID(layers.Layer):\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, a = inputs\n",
        "            mask = K.argmax(a, 1)\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = K.sqrt(K.sum(K.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = K.argmax(x, 1)\n",
        "\n",
        "        increasing = tf.range(start=0, limit=tf.shape(inputs)[0], delta=1)\n",
        "        m = tf.stack([increasing, tf.cast(mask, tf.int32)], axis=1)\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        # x1 = tf.transpose(inputs, (0))\n",
        "        masked = tf.gather_nd(inputs, m)\n",
        "\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape[0], tuple):  # true label provided\n",
        "            return tuple([None, input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[2]])\n",
        "    def get_config(self):\n",
        "       \n",
        "        base_config = super(Mask_CID, self).get_config()\n",
        "        return base_config\n",
        "\n",
        "\n",
        "class ConvCapsuleLayer3D(layers.Layer):\n",
        "\n",
        "    def __init__(self, kernel_size, num_capsule, num_atoms, strides=1, padding='valid', routings=3,\n",
        "                 kernel_initializer='he_normal', **kwargs):\n",
        "        super(ConvCapsuleLayer3D, self).__init__(**kwargs)\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_capsule = num_capsule\n",
        "        self.num_atoms = num_atoms\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 5, \"The input Tensor should have shape=[None, input_height, input_width,\" \\\n",
        "                                      \" input_num_capsule, input_num_atoms]\"\n",
        "        self.input_height = input_shape[1]\n",
        "        self.input_width = input_shape[2]\n",
        "        self.input_num_capsule = input_shape[3]\n",
        "        self.input_num_atoms = input_shape[4]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.input_num_atoms, self.kernel_size, self.kernel_size, 1, self.num_capsule * self.num_atoms],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.b = self.add_weight(shape=[self.num_capsule, self.num_atoms, 1, 1],\n",
        "                                 initializer=initializers.constant(0.1),\n",
        "                                 name='b')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, input_tensor, training=None):\n",
        "\n",
        "        input_transposed = tf.transpose(input_tensor, [0, 3, 4, 1, 2])\n",
        "        input_shape = K.shape(input_transposed)\n",
        "        input_tensor_reshaped = K.reshape(input_tensor, [input_shape[0], 1, self.input_num_capsule * self.input_num_atoms, self.input_height, self.input_width])\n",
        "\n",
        "        input_tensor_reshaped.set_shape((None, 1, self.input_num_capsule * self.input_num_atoms, self.input_height, self.input_width))\n",
        "\n",
        "        # conv = Conv3D(input_tensor_reshaped, self.W, (self.strides, self.strides),\n",
        "        #                 padding=self.padding, data_format='channels_first')\n",
        "\n",
        "        conv = K.conv3d(input_tensor_reshaped, self.W, strides=(self.input_num_atoms, self.strides, self.strides), padding=self.padding, data_format='channels_first')\n",
        "\n",
        "        votes_shape = K.shape(conv)\n",
        "        _, _, _, conv_height, conv_width = conv.get_shape()\n",
        "        conv = tf.transpose(conv, [0, 2, 1, 3, 4])\n",
        "        votes = K.reshape(conv, [input_shape[0], self.input_num_capsule, self.num_capsule, self.num_atoms, votes_shape[3], votes_shape[4]])\n",
        "        votes.set_shape((None, self.input_num_capsule, self.num_capsule, self.num_atoms, conv_height, conv_width))\n",
        "        # votes.set_shape((None, self.input_num_capsule, self.num_capsule, self.num_atoms, conv_height.value, conv_width.value))\n",
        "\n",
        "        logit_shape = K.stack([input_shape[0], self.input_num_capsule, self.num_capsule, votes_shape[3], votes_shape[4]])\n",
        "        biases_replicated = K.tile(self.b, [1, 1, conv_height, conv_width])\n",
        "        # biases_replicated = K.tile(self.b, [1, 1, conv_height.value, conv_width.value])\n",
        "\n",
        "        activations = update_routing(\n",
        "            votes=votes,\n",
        "            biases=biases_replicated,\n",
        "            logit_shape=logit_shape,\n",
        "            num_dims=6,\n",
        "            input_dim=self.input_num_capsule,\n",
        "            output_dim=self.num_capsule,\n",
        "            num_routing=self.routings)\n",
        "\n",
        "        a2 = tf.transpose(activations, [0, 3, 4, 1, 2])\n",
        "        return a2\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        space = input_shape[1:-2]\n",
        "        new_space = []\n",
        "        for i in range(len(space)):\n",
        "            new_dim = conv_output_length(space[i], self.kernel_size, padding=self.padding, stride=self.strides, dilation=1)\n",
        "            new_space.append(new_dim)\n",
        "\n",
        "        return (input_shape[0],) + tuple(new_space) + (self.num_capsule, self.num_atoms)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'num_atoms': self.num_atoms,\n",
        "            'strides': self.strides,\n",
        "            'padding': self.padding,\n",
        "            'routings': self.routings,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer)\n",
        "        }\n",
        "        base_config = super(ConvCapsuleLayer3D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def update_routing(votes, biases, logit_shape, num_dims, input_dim, output_dim,\n",
        "                   num_routing):\n",
        "    if num_dims == 6:\n",
        "        votes_t_shape = [3, 0, 1, 2, 4, 5]\n",
        "        r_t_shape = [1, 2, 3, 0, 4, 5]\n",
        "    elif num_dims == 4:\n",
        "        votes_t_shape = [3, 0, 1, 2]\n",
        "        r_t_shape = [1, 2, 3, 0]\n",
        "    else:\n",
        "        raise NotImplementedError('Not implemented')\n",
        "\n",
        "    votes_trans = tf.transpose(votes, votes_t_shape)\n",
        "    _, _, _, height, width, caps = votes_trans.get_shape()\n",
        "\n",
        "    def _body(i, logits, activations):\n",
        "        \"\"\"Routing while loop.\"\"\"\n",
        "        # route: [batch, input_dim, output_dim, ...]\n",
        "        a,b,c,d,e = logits.get_shape()\n",
        "        a = logit_shape[0]\n",
        "        b = logit_shape[1]\n",
        "        c = logit_shape[2]\n",
        "        d = logit_shape[3]\n",
        "        e = logit_shape[4]\n",
        "        print(logit_shape)\n",
        "        logit_temp = tf.reshape(logits, [a,b,-1])\n",
        "        route_temp = tf.nn.softmax(logit_temp, axis=-1)\n",
        "        route = tf.reshape(route_temp, [a, b, c, d, e])\n",
        "        preactivate_unrolled = route * votes_trans\n",
        "        preact_trans = tf.transpose(preactivate_unrolled, r_t_shape)\n",
        "        preactivate = tf.reduce_sum(preact_trans, axis=1) + biases\n",
        "        activation = _squash(preactivate)\n",
        "        activations = activations.write(i, activation)\n",
        "\n",
        "        act_3d = K.expand_dims(activation, 1)\n",
        "        tile_shape = np.ones(num_dims, dtype=np.int32).tolist()\n",
        "        tile_shape[1] = input_dim\n",
        "        act_replicated = tf.tile(act_3d, tile_shape)\n",
        "        distances = tf.reduce_sum(votes * act_replicated, axis=3)\n",
        "        logits += distances\n",
        "        return (i + 1, logits, activations)\n",
        "\n",
        "    activations = tf.TensorArray(\n",
        "        dtype=tf.float32, size=num_routing, clear_after_read=False)\n",
        "    logits = tf.fill(logit_shape, 0.0)\n",
        "\n",
        "    # shape_invariants_var = [tf.TensorShape([]),\n",
        "    #                          logits.get_shape(),\n",
        "    #                         votes_trans.get_shape()]\n",
        "\n",
        "    i = tf.constant(0, dtype=tf.int32)\n",
        "    _, logits, activations = tf.while_loop(\n",
        "        lambda i, logits, activations: i < num_routing,\n",
        "        _body,\n",
        "        loop_vars=[i, logits, activations],\n",
        "        swap_memory=True)\n",
        "    a = K.cast(activations.read(num_routing - 1), dtype='float32')\n",
        "    return K.cast(activations.read(num_routing - 1), dtype='float32')\n",
        "\n",
        "\n",
        "class DenseCaps(layers.Layer):\n",
        "\n",
        "    def __init__(self, ch_j, n_j,\n",
        "                 r_num=1,\n",
        "                 b_alphas=[8, 8, 8],\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 kernel_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(DenseCaps, self).__init__(**kwargs)\n",
        "        self.ch_j = ch_j  # number of capsules in layer J\n",
        "        self.n_j = n_j  # number of neurons in a capsule in J\n",
        "        self.r_num = r_num\n",
        "        self.b_alphas = b_alphas\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
        "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
        "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
        "        self.input_spec = InputSpec(min_ndim=3)\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.ch_i, self.n_i = input_shape[1:]\n",
        "\n",
        "        self.w_shape = (self.ch_i, self.n_i, self.ch_j, self.n_j)\n",
        "\n",
        "        self.w = self.add_weight(shape=self.w_shape,\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='kernel',\n",
        "                                 regularizer=self.kernel_regularizer,\n",
        "                                 constraint=self.kernel_constraint)\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.r_num == 1:\n",
        "            outputs = K.dot(K.reshape(inputs, (-1, self.ch_i * self.n_i)),\n",
        "                            K.reshape(self.w, (self.ch_i * self.n_i,\n",
        "                                               self.ch_j * self.n_j)))\n",
        "            outputs = squeeze(K.reshape(outputs, (-1, self.ch_j, self.n_j)))\n",
        "        else:\n",
        "            wr = K.reshape(self.w, (self.ch_i, self.n_i, self.ch_j * self.n_j))\n",
        "\n",
        "            u = tf.transpose(tf.matmul(tf.transpose(inputs, [1, 0, 2]), wr), [1, 0, 2])\n",
        "\n",
        "            u = K.reshape(u, (-1, self.ch_i, self.ch_j, self.n_j))\n",
        "\n",
        "            def rt(ub):\n",
        "                ub = K.reshape(ub, (-1, self.ch_i, self.ch_j, self.n_j))\n",
        "                ub_wo_g = K.stop_gradient(ub)\n",
        "                b = 0.0\n",
        "                for r in range(self.r_num):\n",
        "                    if r > 0:\n",
        "                        c = K.expand_dims(K.softmax(b * self.b_alphas[r])) * self.ch_j  # distribution of weighs of capsules in I across capsules in J\n",
        "                        c = K.stop_gradient(c)\n",
        "                    else:\n",
        "                        c = 1.0\n",
        "\n",
        "                    if r == self.r_num - 1:\n",
        "                        cub = c * ub\n",
        "                    else:\n",
        "                        cub = c * ub_wo_g\n",
        "                    s = K.sum(cub, axis=-3)  # vectors of capsules in J\n",
        "                    v = squeeze(s)  # squeezed vectors of capsules in J\n",
        "                    if r == self.r_num - 1:\n",
        "                        break\n",
        "\n",
        "                    v = K.stop_gradient(v)\n",
        "\n",
        "                    a = tf.einsum('bjk,bijk->bij', v, ub)  # a = v dot u\n",
        "                    # a = K.matmul(K.reshape(v, (-1, 1, J, 1, n_j)),\n",
        "                    #             K.reshape(u, (-1, I, J, n_j, 1))).reshape((-1, I, J))\n",
        "\n",
        "                    b = b + a  # increase those b[i,j] where v[j] dot b[i,j] is larger\n",
        "                return v\n",
        "\n",
        "            u = K.reshape(u, (-1, self.ch_i * self.ch_j * self.n_j))\n",
        "\n",
        "            global useGPU\n",
        "\n",
        "            if useGPU:\n",
        "                outputs = rt(u)\n",
        "            else:\n",
        "                outputs = tf.map_fn(rt, u,\n",
        "                                    parallel_iterations=100, back_prop=True,\n",
        "                                    infer_shape=False)\n",
        "\n",
        "            outputs = K.reshape(outputs, (-1, self.ch_j, self.n_j))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.ch_j, self.n_j)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'ch_j': self.ch_j,\n",
        "            'n_j': self.n_j,\n",
        "            'r_num': self.r_num,\n",
        "            'b_alphas': self.b_alphas,\n",
        "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n",
        "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "        }\n",
        "        base_config = super(DenseCaps, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, num_capsule, dim_capsule, channels, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.channels = channels\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        if(self.channels != 0):\n",
        "            assert int(self.input_num_capsule / self.channels) / (self.input_num_capsule / self.channels) == 1, \"error\"\n",
        "            self.W = self.add_weight(shape=[self.num_capsule, self.channels,\n",
        "                                            self.dim_capsule, self.input_dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='W')\n",
        "\n",
        "            self.B = self.add_weight(shape=[self.num_capsule, self.dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='B')\n",
        "        else:\n",
        "            self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                            self.dim_capsule, self.input_dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='W')\n",
        "            self.B = self.add_weight(shape=[self.num_capsule, self.dim_capsule],\n",
        "                                     initializer=self.kernel_initializer,\n",
        "                                     name='B')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        inputs_expand = K.expand_dims(inputs, 1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        inputs_tiled = K.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
        "\n",
        "        if(self.channels != 0):\n",
        "            W2 = K.repeat_elements(self.W, int(self.input_num_capsule / self.channels), 1)\n",
        "        else:\n",
        "            W2 = self.W\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "\n",
        "\n",
        "        inputs_hat = K.map_fn(lambda x: K.batch_dot(x, W2, [2, 3]), elems=inputs_tiled)\n",
        "\n",
        "        # inputs_hat = K.map_fn(lambda x: own_batch_dot(x, W2, [2, 3]), elems=inputs_tiled)\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.num_capsule, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis = 0)\n",
        "            print(c.shape)\n",
        "            print(inputs_hat.shape)\n",
        "            print(b.shape)\n",
        "            print(K.batch_dot(c, inputs_hat, [2, 2]))\n",
        "            print(self.B.shape)\n",
        "\n",
        "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
        "            \n",
        "            outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]) + self.B)  # [None, 10, 16]\n",
        "            # outputs = squash(K.batch_dot(c, inputs_hat, [2, 2]))  \n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                b += K.batch_dot(outputs, inputs_hat, [2, 3])\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule':self.num_capsule, \n",
        "        'dim_capsule': self.dim_capsule,\n",
        "        'routings':self.routings,\n",
        "        'channels':self.channels ,\n",
        "        'kernel_initializer': initializers.serialize(self.kernel_initializer)\n",
        "          }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def _squash(input_tensor):\n",
        "    norm = tf.norm(input_tensor, axis=-1, keepdims=True)\n",
        "    norm_squared = norm * norm\n",
        "    return (input_tensor / norm) * (norm_squared / (1 + norm_squared))\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)\n",
        "    return scale * vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YDltNvdRnKq"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import layers, models, optimizers\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Conv2D, Activation, Dense, Dropout, Lambda, Reshape, Concatenate\n",
        "from keras.layers import BatchNormalization, MaxPooling2D, Flatten, Conv1D, Deconvolution2D, Conv2DTranspose\n",
        "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def BaseCapsNet(input_shape, n_class, routings):\n",
        "    # assemble encoder\n",
        "    x = Input(shape=input_shape)\n",
        "    l = x\n",
        "\n",
        "    l = Conv2D(256, (9, 9), strides=(2, 2), activation='relu', padding=\"same\")(l) \n",
        "    l = BatchNormalization()(l)\n",
        "    l = Conv2D(256, (9, 9), strides=(2, 2), activation='relu', padding=\"same\")(l) \n",
        "    l = BatchNormalization()(l)\n",
        "    l = ConvertToCaps()(l)\n",
        "\n",
        "    # l = Conv2DCaps(16, 6, kernel_size=(3, 3), strides=(2, 2), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(2, 2), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l_skip = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = layers.Add()([l, l_skip])\n",
        "    l1 = l\n",
        "\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(2, 2), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l_skip = ConvCapsuleLayer3D(kernel_size=3, num_capsule=32, num_atoms=8, strides=1, padding='same', routings=3)(l)\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = layers.Add()([l, l_skip])\n",
        "    l2 = l\n",
        "\n",
        "\n",
        "    la = FlattenCaps()(l2)\n",
        "    lb = FlattenCaps()(l1)\n",
        "    l = layers.Concatenate(axis=-2)([la, lb])\n",
        "\n",
        "    digits_caps = CapsuleLayer(num_capsule=2, dim_capsule=10, routings=routings, channels=0, name='digit_caps')(l)\n",
        "    l = CapsToScalars(name='capsnet')(digits_caps)\n",
        "\n",
        "    # m_capsnet = Model(inputs=x, outputs=l, name='capsnet_model')\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "\n",
        "    # masked_by_y = Mask()([digits_caps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digits_caps)\n",
        "\n",
        "    densel = Dense(16, activation = \"relu\")(l)\n",
        "    # flatten = Flatten()(dense1)\n",
        "    output = Dense(2, activation='softmax')(densel)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([x], [output])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(x, [output])\n",
        "\n",
        "\n",
        "    # Decoder Network\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(Dense(input_dim=80, activation=\"relu\", output_dim=8 * 8 * 16))\n",
        "    # decoder.add(Reshape((8, 8, 16)))\n",
        "    # decoder.add(BatchNormalization(momentum=0.8))\n",
        "    # decoder.add(layers.Deconvolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
        "    # decoder.add(layers.Deconvolution2D(32, 3, 3, subsample=(2, 2), border_mode='same'))\n",
        "    # decoder.add(layers.Deconvolution2D(16, 3, 3, subsample=(2, 2), border_mode='same'))\n",
        "    # decoder.add(layers.Deconvolution2D(3, 3, 3, subsample=(1, 1), border_mode='same'))\n",
        "    # decoder.add(Activation(\"relu\"))\n",
        "    # decoder.add(layers.Reshape(target_shape=(32, 32, 3), name='out_recon'))\n",
        "\n",
        "    # train_model = models.Model([x, y], [m_capsnet.output, decoder(masked_by_y)])\n",
        "    # eval_model = models.Model(x, [m_capsnet.output, decoder(masked)])\n",
        "    train_model.summary()\n",
        "\n",
        "    return train_model, eval_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvMdqK54Ry55",
        "outputId": "9ba3e8b8-3376-48a0-d821-501cc71eb443"
      },
      "source": [
        "model, eval_model = BaseCapsNet(input_shape=(256,256,3), n_class=2, routings=3)  #for 28*28"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "Tensor(\"conv_capsule_layer3d_1/stack:0\", shape=(5,), dtype=int32)\n",
            "(?, 2, 40960)\n",
            "(?, 2, 40960, 10)\n",
            "(?, 2, 40960)\n",
            "Tensor(\"digit_caps/Squeeze:0\", shape=(?, 2, 10), dtype=float32)\n",
            "(2, 10)\n",
            "(?, 2, 40960)\n",
            "(?, 2, 40960, 10)\n",
            "(?, 2, 40960)\n",
            "Tensor(\"digit_caps/Squeeze_3:0\", shape=(?, 2, 10), dtype=float32)\n",
            "(2, 10)\n",
            "(?, 2, 40960)\n",
            "(?, 2, 40960, 10)\n",
            "(?, 2, 40960)\n",
            "Tensor(\"digit_caps/Squeeze_6:0\", shape=(?, 2, 10), dtype=float32)\n",
            "(2, 10)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 256 62464       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 128, 128, 256 1024        conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 256)  5308672     batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64, 64, 256)  1024        conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "convert_to_caps_1 (ConvertToCap (None, 64, 64, 256,  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_1 (Conv2DCaps)      (None, 32, 32, 32, 8 589824      convert_to_caps_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_3 (Conv2DCaps)      (None, 32, 32, 32, 8 589824      conv2d_caps_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_4 (Conv2DCaps)      (None, 32, 32, 32, 8 589824      conv2d_caps_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_2 (Conv2DCaps)      (None, 32, 32, 32, 8 589824      conv2d_caps_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 32, 8 0           conv2d_caps_4[0][0]              \n",
            "                                                                 conv2d_caps_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_5 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_6 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_7 (Conv2DCaps)      (None, 16, 16, 32, 8 589824      conv2d_caps_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_capsule_layer3d_1 (ConvCap (None, 16, 16, 32, 8 18688       conv2d_caps_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 16, 16, 32, 8 0           conv2d_caps_7[0][0]              \n",
            "                                                                 conv_capsule_layer3d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "flatten_caps_1 (FlattenCaps)    (None, 8192, 8)      0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_caps_2 (FlattenCaps)    (None, 32768, 8)     0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 40960, 8)     0           flatten_caps_1[0][0]             \n",
            "                                                                 flatten_caps_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "digit_caps (CapsuleLayer)       (None, 2, 10)        6553620     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (CapsToScalars)         (None, 2)            0           digit_caps[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 16)           48          capsnet[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            34          dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 16,074,342\n",
            "Trainable params: 16,073,318\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB8T7dp8SSRB",
        "outputId": "5ca8ebfd-35a3-44c8-b7a8-0711f1aeb426"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8hk5-E_SYfh",
        "outputId": "60890718-aca7-42e0-c945-f39a26b54050"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,steps_per_epoch=int(1933 / 32),\n",
        "                    epochs=15)\n",
        "\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_epoch15New.h5\")\n",
        "\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator, steps=int(646 / 32)+1)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 117s 2s/step - loss: 0.6580 - acc: 0.6224\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.5922 - acc: 0.6511\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 98s 2s/step - loss: 0.5543 - acc: 0.6624\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 98s 2s/step - loss: 0.5088 - acc: 0.7676\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 98s 2s/step - loss: 0.4756 - acc: 0.8098\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 98s 2s/step - loss: 0.4442 - acc: 0.8275\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 98s 2s/step - loss: 0.4424 - acc: 0.8247\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 98s 2s/step - loss: 0.4058 - acc: 0.8378\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.3889 - acc: 0.8522\n",
            "Epoch 10/15\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.3848 - acc: 0.8546\n",
            "Epoch 11/15\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.3913 - acc: 0.8394\n",
            "Epoch 12/15\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.3794 - acc: 0.8439\n",
            "Epoch 13/15\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3562 - acc: 0.8625\n",
            "Epoch 14/15\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.3473 - acc: 0.8608\n",
            "Epoch 15/15\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.3325 - acc: 0.8738\n",
            "[[408  12]\n",
            " [ 71 155]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.97      0.91       420\n",
            "           1       0.93      0.69      0.79       226\n",
            "\n",
            "    accuracy                           0.87       646\n",
            "   macro avg       0.89      0.83      0.85       646\n",
            "weighted avg       0.88      0.87      0.87       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v-gFtWOYb-r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BScRWRtTYbHr"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras import layers, models, optimizers\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Conv2D, Activation, Dense, Dropout, Lambda, Reshape, Concatenate\n",
        "from keras.layers import BatchNormalization, MaxPooling2D, Flatten, Conv1D, Deconvolution2D, Conv2DTranspose\n",
        "from keras.callbacks import Callback, ModelCheckpoint, TensorBoard\n",
        "from keras.utils import plot_model\n",
        "from keras.layers.convolutional import UpSampling2D\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def BaseCapsNet(input_shape, n_class, routings):\n",
        "    # assemble encoder\n",
        "    x = Input(shape=input_shape)\n",
        "    l = x\n",
        "\n",
        "    l = Conv2D(256, (9, 9), strides=(2, 2), activation='relu', padding=\"same\")(l) \n",
        "    l = BatchNormalization()(l)\n",
        "    l = Conv2D(256, (9, 9), strides=(2, 2), activation='relu', padding=\"same\")(l) \n",
        "    l = BatchNormalization()(l)\n",
        "    l = ConvertToCaps()(l)\n",
        "\n",
        "    # l = Conv2DCaps(16, 6, kernel_size=(3, 3), strides=(2, 2), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(2, 2), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l_skip = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = Conv2DCaps(32, 8, kernel_size=(3, 3), strides=(1, 1), r_num=1, b_alphas=[1, 1, 1])(l)\n",
        "    l = layers.Add()([l, l_skip])\n",
        "    l1 = l\n",
        "\n",
        "   \n",
        "    \n",
        "    lb = FlattenCaps()(l1)\n",
        "    \n",
        "\n",
        "    digits_caps = CapsuleLayer(num_capsule=2, dim_capsule=10, routings=routings, channels=0, name='digit_caps')(lb)\n",
        "    l = CapsToScalars(name='capsnet')(digits_caps)\n",
        "\n",
        "    # m_capsnet = Model(inputs=x, outputs=l, name='capsnet_model')\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "\n",
        "    # masked_by_y = Mask()([digits_caps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digits_caps)\n",
        "\n",
        "    densel = Dense(16, activation = \"relu\")(l)\n",
        "    # flatten = Flatten()(dense1)\n",
        "    output = Dense(2, activation='softmax')(densel)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([x], [output])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(x, [output])\n",
        "\n",
        "\n",
        "    # Decoder Network\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(Dense(input_dim=80, activation=\"relu\", output_dim=8 * 8 * 16))\n",
        "    # decoder.add(Reshape((8, 8, 16)))\n",
        "    # decoder.add(BatchNormalization(momentum=0.8))\n",
        "    # decoder.add(layers.Deconvolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
        "    # decoder.add(layers.Deconvolution2D(32, 3, 3, subsample=(2, 2), border_mode='same'))\n",
        "    # decoder.add(layers.Deconvolution2D(16, 3, 3, subsample=(2, 2), border_mode='same'))\n",
        "    # decoder.add(layers.Deconvolution2D(3, 3, 3, subsample=(1, 1), border_mode='same'))\n",
        "    # decoder.add(Activation(\"relu\"))\n",
        "    # decoder.add(layers.Reshape(target_shape=(32, 32, 3), name='out_recon'))\n",
        "\n",
        "    # train_model = models.Model([x, y], [m_capsnet.output, decoder(masked_by_y)])\n",
        "    # eval_model = models.Model(x, [m_capsnet.output, decoder(masked)])\n",
        "    train_model.summary()\n",
        "\n",
        "    return train_model, eval_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViTVvSFCYmO9",
        "outputId": "853c2d9b-3e27-4bc5-c7c9-69897117f79d"
      },
      "source": [
        "model, eval_model = BaseCapsNet(input_shape=(256,256,3), n_class=2, routings=3)  #for 28*28"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 2, 32768)\n",
            "(?, 2, 32768, 10)\n",
            "(?, 2, 32768)\n",
            "Tensor(\"digit_caps_1/Squeeze:0\", shape=(?, 2, 10), dtype=float32)\n",
            "(2, 10)\n",
            "(?, 2, 32768)\n",
            "(?, 2, 32768, 10)\n",
            "(?, 2, 32768)\n",
            "Tensor(\"digit_caps_1/Squeeze_3:0\", shape=(?, 2, 10), dtype=float32)\n",
            "(2, 10)\n",
            "(?, 2, 32768)\n",
            "(?, 2, 32768, 10)\n",
            "(?, 2, 32768)\n",
            "Tensor(\"digit_caps_1/Squeeze_6:0\", shape=(?, 2, 10), dtype=float32)\n",
            "(2, 10)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 128, 128, 256 62464       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 128, 128, 256 1024        conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 256)  5308672     batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "convert_to_caps_2 (ConvertToCap (None, 64, 64, 256,  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_8 (Conv2DCaps)      (None, 32, 32, 32, 8 589824      convert_to_caps_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_10 (Conv2DCaps)     (None, 32, 32, 32, 8 589824      conv2d_caps_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_11 (Conv2DCaps)     (None, 32, 32, 32, 8 589824      conv2d_caps_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_caps_9 (Conv2DCaps)      (None, 32, 32, 32, 8 589824      conv2d_caps_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 32, 8 0           conv2d_caps_11[0][0]             \n",
            "                                                                 conv2d_caps_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_caps_3 (FlattenCaps)    (None, 32768, 8)     0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "digit_caps (CapsuleLayer)       (None, 2, 10)        5242900     flatten_caps_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (CapsToScalars)         (None, 2)            0           digit_caps[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 16)           48          capsnet[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 2)            34          dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 12,975,462\n",
            "Trainable params: 12,974,438\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihl9Z9kBYqzJ"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRHttNAxYtwh",
        "outputId": "0051ede6-4a23-4125-cea2-eafb87f8209e"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,steps_per_epoch=int(1933 / 32),\n",
        "                    epochs=15)\n",
        "\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_epoch15New-noparallel.h5\")\n",
        "\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator, steps=int(646 / 32)+1)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 95s 2s/step - loss: 0.6361 - acc: 0.6510\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.6095 - acc: 0.6509\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.5606 - acc: 0.6554\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.5252 - acc: 0.6886\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 91s 2s/step - loss: 0.5005 - acc: 0.7350\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.4600 - acc: 0.7955\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 91s 2s/step - loss: 0.4278 - acc: 0.8217\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.4097 - acc: 0.8319\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.3667 - acc: 0.8561\n",
            "Epoch 10/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.3386 - acc: 0.8762\n",
            "Epoch 11/15\n",
            "60/60 [==============================] - 91s 2s/step - loss: 0.3462 - acc: 0.8650\n",
            "Epoch 12/15\n",
            "60/60 [==============================] - 92s 2s/step - loss: 0.3228 - acc: 0.8733\n",
            "Epoch 13/15\n",
            "60/60 [==============================] - 94s 2s/step - loss: 0.3124 - acc: 0.8832\n",
            "Epoch 14/15\n",
            "60/60 [==============================] - 91s 2s/step - loss: 0.2798 - acc: 0.8946\n",
            "Epoch 15/15\n",
            "60/60 [==============================] - 94s 2s/step - loss: 0.2630 - acc: 0.9054\n",
            "[[399  21]\n",
            " [ 61 165]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.95      0.91       420\n",
            "           1       0.89      0.73      0.80       226\n",
            "\n",
            "    accuracy                           0.87       646\n",
            "   macro avg       0.88      0.84      0.85       646\n",
            "weighted avg       0.87      0.87      0.87       646\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S04nZOCD2ast"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "o83THVeJQWfb",
        "outputId": "5425d69e-def2-48ab-b372-43ecabbe4d2d"
      },
      "source": [
        "\n",
        "  \n",
        "\"\"\"\n",
        "Some key layers used for constructing a Capsule Network. These layers can used to construct CapsNet on other dataset,\n",
        "not just on MNIST.\n",
        "*NOTE*: some functions can be implemented in multiple ways, I keep all of them. You can try them for yourself just by\n",
        "uncommenting them and commenting their counterparts.\n",
        "Author: Xifeng Guo, E-mail: `guoxifeng1990@163.com`, Github: `https://github.com/XifengGuo/CapsNet-Keras`\n",
        "\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras import initializers, layers\n",
        "\n",
        "\n",
        "class Length(layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss.\n",
        "    Using this layer as model's output can directly predict labels by using `y_pred = np.argmax(model.predict(x), 1)`\n",
        "    inputs: shape=[None, num_vectors, dim_vector]\n",
        "    output: shape=[None, num_vectors]\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return tf.sqrt(tf.reduce_sum(tf.square(inputs), -1) + K.epsilon())\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:-1]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Length, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "class Mask(layers.Layer):\n",
        "    \"\"\"\n",
        "    Mask a Tensor with shape=[None, num_capsule, dim_vector] either by the capsule with max length or by an additional\n",
        "    input mask. Except the max-length capsule (or specified capsule), all vectors are masked to zeros. Then flatten the\n",
        "    masked Tensor.\n",
        "    For example:\n",
        "        ```\n",
        "        x = keras.layers.Input(shape=[8, 3, 2])  # batch_size=8, each sample contains 3 capsules with dim_vector=2\n",
        "        y = keras.layers.Input(shape=[8, 3])  # True labels. 8 samples, 3 classes, one-hot coding.\n",
        "        out = Mask()(x)  # out.shape=[8, 6]\n",
        "        # or\n",
        "        out2 = Mask()([x, y])  # out2.shape=[8,6]. Masked with true labels y. Of course y can also be manipulated.\n",
        "        ```\n",
        "    \"\"\"\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if type(inputs) is list:  # true label is provided with shape = [None, n_classes], i.e. one-hot code.\n",
        "            assert len(inputs) == 2\n",
        "            inputs, mask = inputs\n",
        "        else:  # if no true label, mask by the max length of capsules. Mainly used for prediction\n",
        "            # compute lengths of capsules\n",
        "            x = tf.sqrt(tf.reduce_sum(tf.square(inputs), -1))\n",
        "            # generate the mask which is a one-hot code.\n",
        "            # mask.shape=[None, n_classes]=[None, num_capsule]\n",
        "            mask = tf.one_hot(indices=tf.argmax(x, 1), depth=x.shape[1])\n",
        "\n",
        "        # inputs.shape=[None, num_capsule, dim_capsule]\n",
        "        # mask.shape=[None, num_capsule]\n",
        "        # masked.shape=[None, num_capsule * dim_capsule]\n",
        "        masked = K.batch_flatten(inputs * tf.expand_dims(mask, -1))\n",
        "        return masked\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if type(input_shape[0]) is tuple:  # true label provided\n",
        "            return tuple([None, input_shape[0][1] * input_shape[0][2]])\n",
        "        else:  # no true label provided\n",
        "            return tuple([None, input_shape[1] * input_shape[2]])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Mask, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n",
        "\n",
        "\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "\n",
        "        # Transform matrix\n",
        "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
        "                                        self.dim_capsule, self.input_dim_capsule],\n",
        "                                 initializer=self.kernel_initializer,\n",
        "                                 name='W')\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
        "        inputs_expand = tf.expand_dims(tf.expand_dims(inputs, 1), -1)\n",
        "\n",
        "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
        "        # inputs_tiled.shape=[None, num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        inputs_tiled = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1, 1])\n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[num_capsule, input_num_capsule, input_dim_capsule]\n",
        "        # W.shape=[num_capsule, input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first two dimensions as `batch` dimension,\n",
        "        # then matmul: [input_dim_capsule] x [dim_capsule, input_dim_capsule]^T -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
        "        inputs_hat = tf.squeeze(tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled))\n",
        "        inputs_hat = tf.tile(tf.expand_dims(inputs_hat,1), [1, self.num_capsule, 1, 1])\n",
        "\n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[inputs.shape[0], self.num_capsule, 1, self.input_num_capsule])\n",
        "\n",
        "        assert self.routings > 0, 'The routings should be > 0.'\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "\n",
        "            # c.shape =  [batch_size, num_capsule, input_num_capsule]\n",
        "            # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "            # The first two dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, num_capsule, dim_capsule]\n",
        "            outputs = squash(tf.matmul(c, inputs_hat))\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, num_capsule, dim_capsule]\n",
        "                # inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule]\n",
        "                # The first two dimensions as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, num_capsule, input_num_capsule]\n",
        "                b += tf.matmul(outputs, inputs_hat, transpose_b=True)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        res = tf.expand_dims(tf.squeeze(outputs),1)\n",
        "        return res\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(CapsuleLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding, i = 0):\n",
        "    \"\"\"\n",
        "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
        "    :param dim_capsule: the dim of the output vector of capsule\n",
        "    :param n_channels: the number of types of capsules\n",
        "    :return: output tensor, shape=[None, num_capsule, dim_capsule]\n",
        "    \"\"\"\n",
        "    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
        "                           name='primarycap_conv2d'+str(i))(inputs)\n",
        "\n",
        "    outputs = layers.Reshape(target_shape=[int(int(output.shape[1]*output.shape[2]*output.shape[3])/dim_capsule), dim_capsule], name='primarycap_reshape'+str(i))(output)\n",
        "    return layers.Lambda(squash, name='primarycap_squash'+str(i))(outputs)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# The following is another way to implement primary capsule layer. This is much slower.\n",
        "# Apply Conv2D `n_channels` times and concatenate all capsules\n",
        "def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\n",
        "    outputs = []\n",
        "    for _ in range(n_channels):\n",
        "        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\n",
        "        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\n",
        "    outputs = layers.Concatenate(axis=1)(outputs)\n",
        "    return layers.Lambda(squash)(outputs)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# The following is another way to implement primary capsule layer. This is much slower.\\n# Apply Conv2D `n_channels` times and concatenate all capsules\\ndef PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):\\n    outputs = []\\n    for _ in range(n_channels):\\n        output = layers.Conv2D(filters=dim_capsule, kernel_size=kernel_size, strides=strides, padding=padding)(inputs)\\n        outputs.append(layers.Reshape([output.get_shape().as_list()[1] ** 2, dim_capsule])(output))\\n    outputs = layers.Concatenate(axis=1)(outputs)\\n    return layers.Lambda(squash)(outputs)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIj5sAURNwkf"
      },
      "source": [
        "K.set_image_data_format('channels_last')\n",
        "\n",
        "\n",
        "def CapsNet(input_shape, n_class, routings):\n",
        "    \"\"\"\n",
        "    A Capsule Network on MNIST.\n",
        "    :param input_shape: data shape, 3d, [width, height, channels]\n",
        "    :param n_class: number of classes\n",
        "    :param routings: number of routing iterations\n",
        "    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n",
        "            `eval_model` can also be used for training.\n",
        "    \"\"\"\n",
        "    Inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Layer 1: Just a conventional Conv2D layer\n",
        "    conv1 = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv1')(Inputs)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    x = layers.Conv2D(filters=32, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv2')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=64, kernel_size=5, strides=1, padding='valid', activation='relu', name='conv3')(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = layers.Conv2D(filters=128, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv4')(x)\n",
        "\n",
        "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n",
        "    primarycaps = PrimaryCap(x, dim_capsule=8, n_channels=3, kernel_size=9, strides=2, padding='valid', name=\"i\", count=1)\n",
        "\n",
        "    primarycaps1 = PrimaryCap(x, dim_capsule=8, n_channels=3, kernel_size=9, strides=2, padding='valid', name=\"j\", count=2)\n",
        "\n",
        "    # capsule = layers.concatenate([primarycaps, primarycaps1], axis=-1)\n",
        "\n",
        "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
        "    # digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                            #  name='digitcaps')(primarycaps)\n",
        "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                             name='digitcaps1')(primarycaps)\n",
        "\n",
        "    digitcaps1 = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n",
        "                             name='digitcaps2')(primarycaps1)\n",
        "\n",
        "    out_caps = layers.Lambda(lambda ls : K.permute_dimensions(layers.concatenate(ls, axis=1), [0,2,1]))([digitcaps,digitcaps1])\n",
        "    \n",
        "    out_caps = layers.Dropout(0, (1, out_caps.get_shape()[2]))(out_caps)\n",
        "\n",
        "    out_caps = layers.Reshape(target_shape=[-1, 32], name='primarycap_reshaped')(out_caps)\n",
        "    \n",
        "\n",
        "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
        "    # If using tensorflow, this will not be necessary. :)\n",
        "    out_caps = Length(name='capsnet')(out_caps)\n",
        "\n",
        "    # Decoder network.\n",
        "    # y = layers.Input(shape=(n_class,))\n",
        "    # masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n",
        "    # masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n",
        "\n",
        "    # # Shared Decoder model in training and prediction\n",
        "    # decoder = models.Sequential(name='decoder')\n",
        "    # decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n",
        "    # decoder.add(layers.Dense(1024, activation='relu'))\n",
        "    # decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n",
        "    # decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n",
        "\n",
        "    # Models for training and evaluation (prediction)\n",
        "    # train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n",
        "    train_model = models.Model([Inputs], [out_caps])\n",
        "    # eval_model = models.Model(x, [out_caps, decoder(masked)])\n",
        "    eval_model = models.Model(Inputs, [out_caps])\n",
        "\n",
        "    # manipulate model\n",
        "    # noise = layers.Input(shape=(n_class, 16))\n",
        "    # noised_digitcaps = layers.Add()([digitcaps, noise])\n",
        "    # masked_noised_y = Mask()([noised_digitcaps, y])\n",
        "    # manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n",
        "    # return train_model, eval_model, manipulate_model\n",
        "    return train_model,eval_model\n",
        "\n",
        "\n",
        "def margin_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
        "    :param y_true: [None, n_classes]\n",
        "    :param y_pred: [None, num_capsule]\n",
        "    :return: a scalar loss value.\n",
        "    \"\"\"\n",
        "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
        "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
        "\n",
        "    return K.mean(K.sum(L, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxULza_V21G1"
      },
      "source": [
        "model, eval_model = CapsNet(input_shape=(256,256,3), n_class=2, routings=3)  #for 28*28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4MlyA4Y26hJ"
      },
      "source": [
        "from keras import optimizers\n",
        "model.compile(optimizer=optimizers.Adam(lr=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  \n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdyZCS5zRoNH",
        "outputId": "14721d39-017d-418e-f384-209ab71f32c5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 252, 252, 16) 1216        input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling2D) (None, 126, 126, 16) 0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 122, 122, 32) 12832       max_pooling2d_22[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling2D) (None, 61, 61, 32)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv3 (Conv2D)                  (None, 57, 57, 64)   51264       max_pooling2d_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling2D) (None, 28, 28, 64)   0           conv3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv4 (Conv2D)                  (None, 20, 20, 128)  663680      max_pooling2d_24[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "i (Conv2D)                      (None, 6, 6, 24)     248856      conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "j (Conv2D)                      (None, 6, 6, 24)     248856      conv4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "ii1 (Reshape)                   (None, 108, 8)       0           i[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "ji2 (Reshape)                   (None, 108, 8)       0           j[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "ij1 (Lambda)                    (None, 108, 8)       0           ii1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "jj2 (Lambda)                    (None, 108, 8)       0           ji2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps1 (CapsuleLayer)       (None, 2, 16)        27648       ij1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "digitcaps2 (CapsuleLayer)       (None, 2, 16)        27648       jj2[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 16, 4)        0           digitcaps1[0][0]                 \n",
            "                                                                 digitcaps2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 16, 4)        0           lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "primarycap_reshaped (Reshape)   (None, 2, 32)        0           dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "capsnet (Length)                (None, 2)            0           primarycap_reshaped[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 1,282,000\n",
            "Trainable params: 1,282,000\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "UgfFSjcF2-h_",
        "outputId": "dc8f879c-5df1-4aee-cc9b-3345ba673025"
      },
      "source": [
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "train_generator=datagen.flow_from_directory(directory=train_dir,batch_size=32)\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history = model.fit_generator(train_generator,steps_per_epoch=int(1933 / 32),\n",
        "                    epochs=15)\n",
        "\n",
        "#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\n",
        "model.save(\"/content/drive/My Drive/capsCovid_epoch15New-multilane.h5\")\n",
        "\n",
        "y_true = test_generator.classes\n",
        "\n",
        "result = model.predict_generator(test_generator, steps=int(646 / 32)+1)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1933 images belonging to 2 classes.\n",
            "Found 646 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "60/60 [==============================] - 352s 6s/step - loss: 0.7166 - acc: 0.6335\n",
            "Epoch 2/15\n",
            "60/60 [==============================] - 66s 1s/step - loss: 0.6109 - acc: 0.6765\n",
            "Epoch 3/15\n",
            "60/60 [==============================] - 60s 1s/step - loss: 0.5090 - acc: 0.7810\n",
            "Epoch 4/15\n",
            "60/60 [==============================] - 59s 987ms/step - loss: 0.4870 - acc: 0.8107\n",
            "Epoch 5/15\n",
            "60/60 [==============================] - 60s 1s/step - loss: 0.4298 - acc: 0.8201\n",
            "Epoch 6/15\n",
            "60/60 [==============================] - 60s 996ms/step - loss: 0.5201 - acc: 0.7359\n",
            "Epoch 7/15\n",
            "60/60 [==============================] - 59s 988ms/step - loss: 0.5182 - acc: 0.7744\n",
            "Epoch 8/15\n",
            "60/60 [==============================] - 60s 1000ms/step - loss: 4.7557 - acc: 0.5826\n",
            "Epoch 9/15\n",
            "60/60 [==============================] - 59s 988ms/step - loss: 7.9712 - acc: 0.5000\n",
            "Epoch 10/15\n",
            "41/60 [===================>..........] - ETA: 19s - loss: 7.9712 - acc: 0.5000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3f2bc69a4b8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Fit the model on the batches generated by datagen.flow().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m history = model.fit_generator(train_generator,steps_per_epoch=int(1933 / 32),\n\u001b[0;32m---> 31\u001b[0;31m                     epochs=15)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#model.save(\"/content/drive/My Drive/capsCovid_epoch10.h5\") renamed in drive to model.save(\"/content/drive/My Drive/capsCovid_epoch15.h5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "bcSq_PNfhbLu",
        "outputId": "3935ed44-fc62-4542-8a8a-021cc836e999"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from keras.models import load_model\n",
        "model = load_model(\"/content/drive/My Drive/capsCovid_2_capsule_epoch15.h5\" , custom_objects={'PrimaryCap': PrimaryCap,'CapsuleLayer':CapsuleLayer,'Length':Length})\n",
        "\n",
        "original_dir = '/content/drive/MyDrive/XRAY'\n",
        "batch_size = 32\n",
        "validation_split = 0.25\n",
        "\n",
        "# all data in train_dir and val_dir which are alias to original_data. (both dir is temporary directory)\n",
        "# don't clear base_dir, because this directory holds on temp directory.\n",
        "base_dir, train_dir, val_dir = train_valid_split(original_dir, validation_split, seed=1)\n",
        "\n",
        "\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "\n",
        "test_generator=datagen.flow_from_directory(directory=val_dir,batch_size=32, shuffle=False)\n",
        "\n",
        "true = []\n",
        "false = []\n",
        "y_pred1 = []\n",
        "y_pred2 = []\n",
        "\n",
        "result = model.predict_generator(test_generator, steps=int(646 / 32)+1)\n",
        "\n",
        "y_pred = np.argmax(result, axis=-1)\n",
        "\n",
        "count=0\n",
        "for i in test_generator.classes:\n",
        "  if i ==0:\n",
        "    false.append(i)\n",
        "    y_pred2.append(y_pred[count])\n",
        "  elif i ==1:\n",
        "    true.append(i)\n",
        "    y_pred1.append(y_pred[count])\n",
        "  count+=1\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(true, y_pred1)\n",
        "tp = tpr_keras\n",
        "print(tpr_keras)\n",
        "\n",
        "fpr_keras, tpr_keras, thresholds_keras = roc_curve(false, y_pred2)\n",
        "fp = fpr_keras\n",
        "print(fpr_keras)\n",
        "auc_rf = auc(fp, tp)\n",
        "plt.plot(fp,tp,label=\"(area = {:.3f})\".format(auc_rf))\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 646 images belonging to 2 classes.\n",
            "[0.         0.93362832 1.        ]\n",
            "[0.         0.02142857 1.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py:800: UndefinedMetricWarning: No negative samples in y_true, false positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}